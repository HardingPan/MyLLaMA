{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19c3e677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from transformers import PretrainedConfig\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93700c4a",
   "metadata": {},
   "source": [
    "# LLaMA2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd255934",
   "metadata": {},
   "source": [
    "## 定义超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b486d086",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig(PretrainedConfig):\n",
    "    model_tyle = \"Tiny-K\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        dim: int = 768,\n",
    "        n_layers: int = 12,\n",
    "        \n",
    "        # 在标准的多头注意力（MHA）中，Query (Q)、Key (K)、Value (V) 的头数 相同（通常是 n_heads）\n",
    "        # GQA (Grouped Query Attention) 的目标是减少计算量和显存\n",
    "        # 让 Query 有更多的头数 n_q_heads，而 Key 和 Value 用更少的头数 n_kv_heads。\n",
    "        # 这样在计算注意力时，Key 和 Value 的存储和计算压力大大降低。\n",
    "        n_heads: int = 16, \n",
    "        n_kv_heads: int = 8,\n",
    "        \n",
    "        # vocab_size 是词汇表的大小，通常用于语言模型的输出层\n",
    "        vocab_size: int = 6144,\n",
    "        hidden_dim: int = None,\n",
    "        multiple_of: int = 64,\n",
    "        norm_eps: float = 1e-5,\n",
    "        max_seq_len: int = 512,\n",
    "        dropout: float = 0.0,\n",
    "        flash_attn: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.dim = dim\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim if hidden_dim is not None else dim * 4\n",
    "        self.multiple_of = multiple_of\n",
    "        self.norm_eps = norm_eps\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.dropout = dropout\n",
    "        self.flash_attn = flash_attn\n",
    "        super().__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6838402a",
   "metadata": {},
   "source": [
    "## RMSNorm 介绍\n",
    "\n",
    "### 1. RMSNorm 的定义\n",
    "**RMSNorm（Root Mean Square Normalization）** 是一种归一化方法，主要用于稳定神经网络的训练过程。  \n",
    "其核心思想是通过向量的均方根（Root Mean Square）来缩放输入，而不是像 LayerNorm 那样减去均值再除以标准差。\n",
    "\n",
    "其公式为：\n",
    "$$\n",
    "\\text{RMSNorm}(x) = \\frac{x}{\\text{RMS}(x)} \\cdot \\gamma\n",
    "$$\n",
    "其中：\n",
    "$$\n",
    "\\text{RMS}(x) = \\sqrt{\\frac{1}{d} \\sum_{i=1}^d x_i^2 + \\epsilon}\n",
    "$$\n",
    "- $x \\in \\mathbb{R}^d$：输入向量。\n",
    "- $\\gamma \\in \\mathbb{R}^d$：可学习的缩放参数。\n",
    "- $\\epsilon$：防止除零的小常数。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. RMSNorm 的作用\n",
    "- **稳定训练**：通过归一化输入的尺度，减少梯度消失或梯度爆炸的问题。\n",
    "- **计算更高效**：不计算均值和标准差，只计算平方和的均方根。\n",
    "- **更适合大模型**：在 LLaMA、GPT-NeoX 等大语言模型中，RMSNorm 被发现比 LayerNorm 更高效，且能保持模型性能。\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 与其他 Norm 的区别\n",
    "\n",
    "#### **3.1 与 LayerNorm 的区别**\n",
    "LayerNorm 公式：\n",
    "$$\n",
    "\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sigma} \\cdot \\gamma + \\beta\n",
    "$$\n",
    "- **均值归一化**：LayerNorm 需要计算 $\\mu$ 和 $\\sigma$。\n",
    "- **偏置项**：LayerNorm 有 $\\beta$（可学习平移参数），RMSNorm 没有。\n",
    "- **中心化**：LayerNorm 会强制输入均值为 0，而 RMSNorm 不会减去均值。\n",
    "\n",
    "对比：\n",
    "| 特性            | LayerNorm            | RMSNorm                      |\n",
    "|----------------|---------------------|------------------------------|\n",
    "| 计算公式        | $(x - \\mu)/\\sigma$  | $x/\\text{RMS}(x)$            |\n",
    "| 计算均值        | 是                  | 否                           |\n",
    "| 偏置项 $\\beta$ | 有                  | 无                           |\n",
    "| 计算复杂度      | 较高                | 较低                         |\n",
    "\n",
    "---\n",
    "\n",
    "#### **3.2 与 BatchNorm / InstanceNorm 的区别**\n",
    "- BatchNorm 和 InstanceNorm 依赖于 **样本维度或批次统计信息**，而 RMSNorm 和 LayerNorm 都是 **逐样本、逐向量** 的归一化。\n",
    "- RMSNorm 的计算量更小，且不需要跟踪 batch 统计信息，非常适合 Transformer 结构。\n",
    "\n",
    "---\n",
    "\n",
    "### 4. 直观理解\n",
    "可以把 RMSNorm 看作是 **一种只做“尺度归一化”的 LayerNorm**：  \n",
    "- LayerNorm = “先中心化再缩放”  \n",
    "- RMSNorm = “只做缩放，不中心化”  \n",
    "\n",
    "这种简化在大模型中不仅节省算力，还能减少数值不稳定性。\n",
    "\n",
    "---\n",
    "\n",
    "### 5. 应用场景\n",
    "- 在 **LLaMA 系列模型** 中，RMSNorm 替代了 LayerNorm，提升了推理速度。\n",
    "- 在 GPT-NeoX 等开源大模型中也被广泛使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22c36ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim)) # weight是一个可学习的参数\n",
    "        \n",
    "    def _nrom(self, x):\n",
    "        # torch.rsqrt 是平方根的倒数\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 训练时，有些模型的输入张量 x 可能是 float16半精度 或 bfloat16\n",
    "        # 因为这可以加快计算速度、节省显存\n",
    "        # 但是在 x.pow(2).mean(-1) 这种操作中, 如果使用 float16 进行平方和平均\n",
    "        # 可能会出现 溢出overflow 或 精度不足, 导致归一化计算不稳定\n",
    "        # 解决办法：在归一化前把数据转换成 float32\n",
    "        # 确保 RMS 计算有足够的数值精度。\n",
    "        output = self._nrom(x.float()).type_as(x)\n",
    "        return output * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dc9d3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过 repeat_kv 把 K、V 的头数复制 n_rep 倍，使其与 Q 匹配\n",
    "def repeat_kv(x: torch.tensor, n_rep: int) -> torch.tensor:\n",
    "    # bs: batch size, slen: sequence length, n_kv_heads: number of key-value heads, \n",
    "    # head_dim: dimension of each head\n",
    "    bs, slen, n_kv_heads, head_dim = x.shape\n",
    "    \n",
    "    if n_rep == 1:\n",
    "        return x # 如果 n_rep 为 1，直接返回原始张量\n",
    "    \n",
    "    return (\n",
    "        x[:, :, :, None, :] # 在 n_kv_heads 和 head_dim 之间插入一个维度 (bs, slen, n_kv_heads, 1, head_dim)\n",
    "        .expand(bs, slen, n_kv_heads, n_rep, head_dim) # 在新插入的维度上复制 n_rep 次\n",
    "        .reshape(bs, slen, n_kv_heads * n_rep, head_dim) # 合并 n_kv_heads 和 n_rep -> n_q_heads\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdb2106",
   "metadata": {},
   "source": [
    "**测试 RMSNorm**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edff5c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelConfig {\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.0,\n",
      "  \"flash_attn\": true,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"max_seq_len\": 512,\n",
      "  \"multiple_of\": 64,\n",
      "  \"n_heads\": 16,\n",
      "  \"n_kv_heads\": 8,\n",
      "  \"n_layers\": 12,\n",
      "  \"norm_eps\": 1e-05,\n",
      "  \"transformers_version\": \"4.54.0\",\n",
      "  \"vocab_size\": 6144\n",
      "}\n",
      "\n",
      "torch.Size([1, 50, 768])\n",
      "tensor(-0.0033, grad_fn=<MeanBackward0>) tensor(1.0000, grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "args = ModelConfig()\n",
    "print(args)\n",
    "norm = RMSNorm(dim=args.dim, eps=args.norm_eps)\n",
    "x = torch.randn(1, 50, args.dim)\n",
    "output = norm(x)\n",
    "print(output.shape)  # 输出形状应为 (1, 50, args.dim)\n",
    "print(output.mean(), output.std())  # 输出均值和标准差"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dda743",
   "metadata": {},
   "source": [
    "## RoPE（Rotary Position Embedding）介绍\n",
    "\n",
    "### 1. RoPE 的定义\n",
    "**RoPE（Rotary Position Embedding）** 是一种相对位置编码方法，用于在 Transformer 的注意力机制中注入位置信息。  \n",
    "与传统的正余弦位置编码（Sinusoidal Positional Encoding）不同，RoPE 通过**在查询（Q）和键（K）向量上应用二维旋转变换**来实现位置感知，从而使模型能够自然地捕捉**相对位置**信息。\n",
    "\n",
    "RoPE 的核心思想是：  \n",
    "将每对偶数维和奇数维的向量视为复数的实部和虚部，然后通过旋转矩阵引入位置信息。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. RoPE 的公式\n",
    "假设 $x = [x_0, x_1, x_2, x_3, \\dots, x_{d-2}, x_{d-1}] \\in \\mathbb{R}^d$，我们将其每两个相邻的维度视为一对：\n",
    "$$\n",
    "(x_{2i}, x_{2i+1}) \\rightarrow (x_r, x_i)\n",
    "$$\n",
    "\n",
    "RoPE 对这对向量应用旋转变换：\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "x'_r \\\\\n",
    "x'_i\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\cos \\theta_p & -\\sin \\theta_p \\\\\n",
    "\\sin \\theta_p & \\cos \\theta_p\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x_r \\\\\n",
    "x_i\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "其中，$\\theta_p$ 是与位置 $p$ 相关的旋转角度，通常根据以下频率公式预先计算：\n",
    "$$\n",
    "\\theta_p = p \\cdot \\omega_i\n",
    "$$\n",
    "$$\n",
    "\\omega_i = 1 / \\theta^{2i/d}\n",
    "$$\n",
    "其中 $\\theta$ 通常为 $10000$，与传统位置编码类似。\n",
    "\n",
    "最终结果是：\n",
    "$$\n",
    "x'_{2i} = x_{2i} \\cos \\theta_p - x_{2i+1} \\sin \\theta_p\n",
    "$$\n",
    "$$\n",
    "x'_{2i+1} = x_{2i} \\sin \\theta_p + x_{2i+1} \\cos \\theta_p\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3. RoPE 的作用\n",
    "- **相对位置感知**：  \n",
    "  RoPE 的旋转机制使得注意力得分 $\\text{Attention}(Q,K)$ 依赖于 $QK^\\top$ 的相对位置差，而非绝对位置。  \n",
    "  这意味着 RoPE 天生具备捕捉相对位置的能力。\n",
    "\n",
    "- **兼容性强**：  \n",
    "  RoPE 直接应用在 Q 和 K 向量上，不改变模型的其他结构，与标准 Transformer 兼容性强。\n",
    "\n",
    "- **数值稳定性和高效性**：  \n",
    "  RoPE 的计算主要是点乘和旋转，计算代价较小。\n",
    "\n",
    "---\n",
    "\n",
    "### 4. RoPE 与传统位置编码的区别\n",
    "\n",
    "| 特性                | 传统 Sinusoidal 编码                | RoPE（旋转位置编码）           |\n",
    "|--------------------|------------------------------------|------------------------------|\n",
    "| **位置信息类型**    | 绝对位置                           | 相对位置                     |\n",
    "| **实现方式**        | 直接加到输入 Embedding 上           | 应用于 Q 和 K 的旋转变换     |\n",
    "| **捕捉长程依赖能力**| 较弱（绝对位置限制）               | 更强（天然支持相对位置差）   |\n",
    "| **额外参数**        | 无                                 | 无                           |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. RoPE 的实现步骤\n",
    "1. **预计算频率（freqs）**  \n",
    "   通过类似于正余弦位置编码的公式计算频率：\n",
    "   $$\n",
    "   \\omega_i = 1 / \\theta^{2i/d}\n",
    "   $$\n",
    "\n",
    "2. **计算位置的正弦与余弦值（freqs_cos, freqs_sin）**  \n",
    "   对于位置 $p$，计算：\n",
    "   $$\n",
    "   \\cos (p \\cdot \\omega_i), \\ \\sin (p \\cdot \\omega_i)\n",
    "   $$\n",
    "\n",
    "3. **将 Q 和 K 分成偶数维与奇数维（实部和虚部）**  \n",
    "   重塑张量形状：  \n",
    "   $$\n",
    "   (\\text{batch}, \\text{seq\\_len}, n_\\text{heads}, head\\_dim) \\rightarrow (\\text{batch}, \\text{seq\\_len}, n_\\text{heads}, head\\_dim//2, 2)\n",
    "   $$\n",
    "\n",
    "\n",
    "4. **应用旋转公式**  \n",
    "   使用：\n",
    "   $$\n",
    "   x'_r = x_r \\cos \\theta - x_i \\sin \\theta\n",
    "   $$\n",
    "   $$\n",
    "   x'_i = x_r \\sin \\theta + x_i \\cos \\theta\n",
    "   $$\n",
    "\n",
    "5. **合并维度还原形状**  \n",
    "   最终恢复到 `(batch, seq_len, n_heads, head_dim)`。\n",
    "\n",
    "---\n",
    "\n",
    "### 6. 应用场景\n",
    "- RoPE 是 LLaMA 系列、ChatGLM 等现代大语言模型的标准位置编码方案。\n",
    "- 相比传统位置编码，它在长文本任务和相对位置敏感的任务上表现更佳。\n",
    "\n",
    "---\n",
    "\n",
    "### 7. 总结\n",
    "- **RoPE 是通过二维旋转将位置信息嵌入 Q 和 K 的头向量中，从而实现对相对位置的自然建模。**\n",
    "- 它不需要额外参数，与 Transformer 完美兼容，并且计算高效。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1988eb8",
   "metadata": {},
   "source": [
    "### 构造获得旋转嵌入的实部和虚部的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc070b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成长度为 end 的序列位置编码信息，用正余弦函数映射到 dim 维空间（通常是 head_dim）\n",
    "# 此处的dim应为 dim//n_head,因为我们是对每个head进行旋转嵌入\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    dim: 向量维度(一般是每个注意力头的维度 head_dim, 即 hidden_dim // n_heads)\n",
    "    end: 最大序列长度(即位置编码要覆盖的 token 数)\n",
    "    theta: 基准频率参数(一般是 10000.0, 和 Transformer 的正余弦位置编码类似)\n",
    "    \"\"\"\n",
    "    # torch.arange(0, dim, 2)\n",
    "    # 生成一个从 0 开始、步长为 2 的序列： [0, 2, 4, ..., dim-2]\n",
    "    # 这里取步长为 2 是因为后续 RoPE 会把向量的偶数维和奇数维看作 二维坐标 (x, y)\n",
    "    # [: (dim // 2)] 取前一半长度，即维度的一半，得到 dim // 2 个频率因子\n",
    "    # .float() / dim 将序列转换为浮点数，并除以 dim，得到一个范围为 [0, 2/dim, 4/dim, ...] 的数列\n",
    "    # theta ** ( ... ) 计算theta^(i/dim)，得到每个频率因子的缩放, 类似于原始 Transformer 中的正余弦位置编码\n",
    "    # 1.0 / (...) 最后取倒数，得到最终的频率值（对应波长的倒数）\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    \n",
    "    # 生成时间（位置）序列: 生成一个从 0 到 end 的序列,⻓度为 end\n",
    "    t = torch.arange(end, device=freqs.device)\n",
    "    \n",
    "    # 计算外积,得到一个二维矩阵,每一行是t的元素乘以freqs的元素\n",
    "    freqs = torch.outer(t, freqs).float() # 得到 每个位置 i 对应的每个频率分量 j 的“相位”\n",
    "    \n",
    "    # 计算频率的余弦值,得到实部\n",
    "    freqs_cos = torch.cos(freqs)\n",
    "    # 计算频率的正弦值,得到虚部\n",
    "    freqs_sin = torch.sin(freqs)\n",
    "    \n",
    "    return freqs_cos, freqs_sin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959847f1",
   "metadata": {},
   "source": [
    "### 构造调整张量形状的函数\n",
    "主要目的是调整 freqs_cis 的形状,使其在进行广播操作时与 x 的维度对对齐,从而能够进行正确的张量运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a32d5f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调整 freqs_cis 的形状，使其能够与 x 在多维张量上正确广播（broadcast），方便进行逐元素的正弦/余弦旋转操作\n",
    "# 这种操作常用于 RoPE（Rotary Position Embedding），因为位置编码 freqs_cis 需要和 Q/K 张量在特定维度对齐\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    freqs_cis: 来自 precompute_freqs_cis 计算的 (seq_len, head_dim) 形状的正弦/余弦矩阵\n",
    "    x: 一般是 Query 或 Key 张量，形状通常为 x.shape=(batch_size,seq_len,n_heads,head_dim)或者(batch_size, seq_len, head_dim)\n",
    "    \"\"\"\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim #  确认 x 至少有 2 个维度\n",
    "    # 确认 freqs_cis 的第一维与 x 的序列长度 seq_len 相同，最后一维与 head_dim 相同\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    \n",
    "    # 构造一个新的形状,除了第二维和最后一维,其他维度都为1,这样做是为了能够将freqs_cis与x进行广播操作\n",
    "    # enumerate(x.shape) 会返回 (i, d)，其中 i 是维度索引，d 是对应的维度大小\n",
    "    # i == 1 or i == ndim - 1：\n",
    "    # 第 1 个维度（序列长度 seq_len）要保留。\n",
    "    # 最后一个维度（head_dim）也要保留\n",
    "    # 其他维度设置为 1, 比如 batch_size 和 n_heads 维度都设为 1，这样 freqs_cis 就能通过广播扩展成和 x 形状相同\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    \n",
    "    return freqs_cis.view(shape) # 调整freqs_cis的形状,使其可以与x进行广播操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ad9ed2",
   "metadata": {},
   "source": [
    "### 实现旋转嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5855b0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cos: torch.Tensor,\n",
    "    freqs_sin: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    xq: Query 张量，形状通常为 (batch_size, seq_len, n_heads, head_dim)\n",
    "    xk: Key 张量，形状通常为 (batch_size, seq_len, n_heads, head_dim)\n",
    "    freqs_cos: 预计算的余弦位置编码，形状为 (seq_len, head_dim)\n",
    "    freqs_sin: 预计算的正弦位置编码，形状为 (seq_len, head_dim)\n",
    "    \n",
    "    返回值是应用 RoPE 后的 Query 和 Key 张量\n",
    "    \"\"\"\n",
    "    xq_r, xq_i = xq.float().reshape(xq.shape[:-1] + (-1, 2)).unbind(dim=-1) # 将 xq 分解为实部和虚部\n",
    "    xk_r, xk_i = xk.float().reshape(xk.shape[:-1] + (-1, 2)).unbind(dim=-1) # 将 xk 分解为实部和虚部\n",
    "    \n",
    "    freqs_cos = reshape_for_broadcast(freqs_cos, xq_r) # 调整 freqs_cos 的形状以便广播\n",
    "    freqs_sin = reshape_for_broadcast(freqs_sin, xq_r) # 调整 freqs_sin 的形状以便广播\n",
    "    \n",
    "    # 应用旋转,分别计算旋转后的实部和虚部\n",
    "    xq_out_r = xq_r * freqs_cos - xq_i * freqs_sin\n",
    "    xq_out_i = xq_r * freqs_sin + xq_i * freqs_cos\n",
    "    xk_out_r = xk_r * freqs_cos - xk_i * freqs_sin\n",
    "    xk_out_i = xk_r * freqs_sin + xk_i * freqs_cos\n",
    "    \n",
    "    # 将最后两个维度合并,并还原为原始张量的形状\n",
    "    xq_out = torch.stack([xq_out_r, xq_out_i], dim=-1).flatten(3)\n",
    "    xk_out = torch.stack([xk_out_r, xk_out_i], dim=-1).flatten(3)\n",
    "    \n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6f4aae",
   "metadata": {},
   "source": [
    "**测试旋转嵌入**："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "246e6220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 24]) torch.Size([50, 24])\n",
      "torch.Size([1, 50, 6, 48]) torch.Size([1, 50, 6, 48])\n"
     ]
    }
   ],
   "source": [
    "xq = torch.randn(1, 50, 6, 48) # bs, seq_len, dim//n_head, n_head_dim\n",
    "xk = torch.randn(1, 50, 6, 48) # bs, seq_len, dim//n_head, n_head_dim\n",
    "\n",
    "# 使用 precompute_freqs_cis 函数获取 sin和cos\n",
    "cos, sin = precompute_freqs_cis(288//6, 50)\n",
    "print(cos.shape, sin.shape)\n",
    "xq_out, xk_out = apply_rotary_emb(xq, xk, cos, sin)\n",
    "print(xq_out.shape, xk_out.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
